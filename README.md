 # ОТЧЁТ ПО КУРСОВОЙ РАБОТЕ
## "Применение методов машинного обучения в токсикологическом анализе"

**Автор:** Астанин Денис  
**Курс:** II семестр обучения  
**Направление:** Классическое машинное обучение  

---

## ВВЕДЕНИЕ

Данная курсовая работа посвящена применению современных методов машинного обучения для анализа биологической активности и токсичности химических соединений. Основная цель исследования - разработка предиктивных моделей для определения токсикологических параметров: цитотоксичности (CC50), активности ингибирования (IC50) и селективности (SI).

Актуальность работы обусловлена необходимостью создания эффективных инструментов для предварительной оценки токсичности новых химических соединений, что позволит сократить количество лабораторных экспериментов и ускорить процесс разработки безопасных препаратов.

---

## 1. РАЗВЕДЫВАТЕЛЬНЫЙ АНАЛИЗ ДАННЫХ (EDA)

### 1.1 Аналитика данных

#### 1.1.1 Описание данных

Исследуемый датасет содержит информацию о биологической активности химических соединений со следующими характеристиками:

- **Общий размер:** 1,001 образец × 214 признаков
- **Общее количество элементов:** 214,214
- **Типы данных:**
  - float64: 107 столбцов (50.2%)
  - int64: 106 столбцов (49.8%)

**Целевые переменные:**
- **IC50 (mM)** - концентрация полумаксимального ингибирования
- **CC50 (mM)** - концентрация полумаксимальной цитотоксичности  
- **SI** - индекс селективности (Selectivity Index)

#### 1.1.2 Первичный анализ данных

**Качество данных:**
- **Пропущенные значения:** 36 (0.02% от общего объёма)
- **Количество столбцов с пропусками:** 12 из 214
- **Проблемные признаки:** MaxPartialCharge, MinPartialCharge, BCUT2D_* серии


| Признак | Количество пропусков | Процент пропусков |
|---------|---------------------|-------------------|
| MaxPartialCharge | 3 | 0.3% |
| MinPartialCharge | 3 | 0.3% |
| MaxAbsPartialCharge | 3 | 0.3% |
| MinAbsPartialCharge | 3 | 0.3% |
| BCUT2D_MWHI | 3 | 0.3% |
| BCUT2D_MWLOW | 3 | 0.3% |

**Обработка пропусков:**
```
Записей до очистки: 1,001
Записей после очистки: 998
Удалено записей: 3 (0.30%)
Пропущенных значений осталось: 0
```

**Анализ дублированных записей:**
```
Полных дубликатов: 32
Дубликатов по признакам (без учета целевых): 196
Записей до удаления дубликатов: 998
Записей после удаления дубликатов: 966
Удалено дубликатов: 32
Общее сокращение данных: 3.50%
```

#### 1.1.3 Анализ распределения целевых переменных

![Распределения целевых переменных](images/target_distributions.png)
*Рисунок 1.2: Распределения биологических параметров (обычное и логарифмическое)*

**Характеристики распределений:**

| Переменная | Тип распределения | Асимметрия | Рекомендация |
|------------|-------------------|------------|--------------|
| IC50, mM   | Логнормальное     | Высокая    | Логарифмирование |
| CC50, mM   | Логнормальное     | Высокая    | Логарифмирование |
| SI         | Логнормальное     | Высокая    | Логарифмирование |

**Выводы по распределениям:**
1. Все целевые переменные имеют логнормальное распределение
2. Логарифмическое преобразование значительно улучшает нормальность  
3. Присутствует большое количество выбросов

#### 1.1.4 Корреляционный анализ целевых и нецелевых признаков

![Матрица корреляций](images/correlation_matrix.png)
*Рисунок 1.3: Матрица корреляций признаков*

**Анализ слабо коррелированных признаков:**
- **IC50:** 11 признаков с корреляцией < 0.01
- **CC50:** 14 признаков с корреляцией < 0.01  
- **SI:** 33 признака с корреляцией < 0.01
- **Общее количество слабо коррелированных признаков:** 47

**Мультиколлинеарность:**
Обнаружены пары признаков с корреляцией > 0.9:
- MaxAbsEStateIndex и MaxEStateIndex: 1.00
- MolWt и HeavyAtomMolWt: 1.00
- MolWt и ExactMolWt: 1.00
- NumValenceElectrons и Chi0: 0.99
- И другие (всего более 50 пар)

### 1.2 Предобработка данных

#### 1.2.1 Анализ выбросов

![Выбросы по признакам](images/outliers_by_features.png)
*Рисунок 1.4: Топ-30 признаков с наибольшим количеством выбросов*

**Статистика выбросов по правилу IQR:**

| Признак | Количество выбросов |
|---------|-------------------|
| Ipc | 218 |
| PEOE_VSA4 | 205 |
| PEOE_VSA13 | 185 |
| PEOE_VSA5 | 166 |
| BCUT2D_MRHI | 153 |
| IC50, mM | 147 |
| SMR_VSA9 | 141 |
| MinEStateIndex | 128 |
| SI | 125 |
| PEOE_VSA11 | 123 |

**Методы обнаружения выбросов:**
1. **Правило 3-х сигм:** μ ± 3σ
2. **Межквартильный размах (IQR):** Q1 - 1.5×IQR, Q3 + 1.5×IQR

#### 1.2.2 Отбор признаков

**Критерии исключения признаков:**
1. **Нулевая дисперсия:** признаки с var() = 0
2. **Высокая корреляция:** r > 0.9 между парами признаков
3. **Слабая связь с целевыми переменными:** |r| < 0.01

**Результат отбора:**
- Исходное количество признаков: 213
- После удаления высококоррелированных: значительное уменьшение размерности
- Финальный набор признаков: оптимизированный для моделирования

#### 1.2.3 Нормализация данных

Применена стандартизация (StandardScaler):
```python
X_scaled = (X - μ) / σ
```

**Результаты предобработки:**
- Создан файл: `processed_data_full.parquet`
- Данные готовы для машинного обучения

### Выводы по EDA

1. **Качество данных высокое** - минимальное количество пропусков (0.02%)
2. **Логарифмическое преобразование целевых переменных** значительно улучшило их распределение
3. **Мультиколлинеарность устранена** удалением высококоррелированных признаков
4. **Выбросы обработаны** по правилу 3-х сигм
5. **Стандартизация** обеспечила сопоставимость масштабов признаков

---

## 2. ОПИСАНИЕ ЭКСПЕРИМЕНТОВ ПО ЗАДАЧАМ МАШИННОГО ОБУЧЕНИЯ

### 2.1 Задачи регрессии

#### 2.1.1 regression_cc50.ipynb - Предсказание CC50

**Предобработка данных:**
- Логарифмическое преобразование: `log₁₀(CC50)`
- Удаление выбросов по правилу 3-х сигм
- Разделение: 80% обучение / 20% тест
- Стандартизация признаков

**Модели и гиперпараметры:**

| Модель | Гиперпараметры | Количество комбинаций |
|--------|----------------|----------------------|
| Ridge | alpha: [0.01, 0.1, 1, 10, 100], solver: ['auto', 'svd', 'cholesky'] | 75 |
| RandomForest | n_estimators: [100, 200], max_depth: [None, 10, 20], min_samples_split: [2, 5] | 60 |
| XGBoost | n_estimators: [100, 200], max_depth: [3, 6], learning_rate: [0.01, 0.1] | 40 |

**Результаты:**

| Модель | R² | RMSE | MAE | CV R² |
|--------|----|----- |----- |-------|
| **RandomForest** | **0.418** | **0.521** | **0.398** | **0.365** |
| XGBoost | 0.385 | 0.535 | 0.412 | 0.362 |
| Ridge | 0.312 | 0.567 | 0.438 | 0.298 |

**Лучшая модель:** RandomForestRegressor
- **Обоснование:** Лучшие показатели R² и наименьшие ошибки RMSE и MAE
- **Хотя XGB и RF близки по cv_mean_r2, RF лучше обобщает на тесте**

#### 2.1.2 regression_ic50.ipynb - Предсказание IC50

**Аналогичная методология:** 
- Логарифмическое преобразование
- Обработка выбросов  
- Те же модели и гиперпараметры
- **Результаты:** Аналогичные CC50 с доминированием RandomForest

#### 2.1.3 regression_si.ipynb - Предсказание SI

**Особенности SI:**
- Наибольшая вариативность среди целевых переменных
- Требует более сложной предобработки
- Логарифмическое преобразование критично
- **Результаты:** RandomForest также показал лучшие результаты

### 2.2 Задачи классификации

#### 2.2.1 classification_cc50_over_median.ipynb

**Предобработка данных:**
- Бинаризация: `CC50_gt_median = (CC50 > median(CC50))`
- Удаление выбросов по исходной переменной  
- Балансировка классов

**Модели и результаты:**

| Модель | ROC-AUC | F1-Score | Precision | Recall | CV ROC-AUC |
|--------|---------|----------|-----------|--------|------------|
| **RandomForest** | **0.876** | **0.871** | **0.869** | **0.873** | **0.834** |
| XGBoost | 0.834 | 0.828 | 0.831 | 0.825 | 0.798 |
| LogisticRegression | 0.792 | 0.785 | 0.788 | 0.782 | 0.756 |

**Лучшие гиперпараметры RandomForest:**
```python
{
    'max_depth': None,
    'min_samples_split': 2, 
    'n_estimators': 200
}
```

**Заключение:** RandomForest классификатор показывает лучшие результаты

#### 2.2.2 classification_si_over_median.ipynb

**Специфика задачи:**
- Исходная переменная: SI  
- Бинарная переменная: `SI_gt_median`
- Принцип разделения: Медианное значение SI как порог
- Интерпретация: Высокая/низкая селективность соединений
- Баланс классов: 0.51 / 0.49

**Результаты:**

| Модель | ROC-AUC | F1-Score | Precision | Recall | CV ROC-AUC |
|--------|---------|----------|-----------|--------|------------|
| **RandomForest** | **0.923** | **0.918** | **0.915** | **0.921** | **0.889** |
| XGBoost | 0.887 | 0.882 | 0.885 | 0.879 | 0.854 |
| LogisticRegression | 0.845 | 0.838 | 0.841 | 0.835 | 0.812 |

**Заключение:** Random Forest классификатор показывает лучшие результаты для SI

#### 2.2.3 classification_si_over_8.ipynb

**Особенность:** Использование фиксированного порога SI > 8 вместо медианы
- Более несбалансированные классы
- Практическое значение порога  
- Аналогичные результаты с доминированием RandomForest

#### 2.2.4 classification_ic50_over_median.ipynb

**Аналогичная методология** для IC50 с результатами, сопоставимыми с CC50

---

## 3. СРАВНИТЕЛЬНЫЙ АНАЛИЗ И ВЫБОР ЛУЧШИХ РЕШЕНИЙ

### 3.1 Сводная таблица результатов

**Регрессионные задачи:**

| Задача | Лучшая модель | R² | RMSE | Качество |
|--------|---------------|----|----- |----------|
| CC50 | RandomForest | 0.418 | 0.521 | Удовлетворительное |
| IC50 | RandomForest | ~0.445 | ~0.498 | Удовлетворительное |
| SI | RandomForest | ~0.382 | ~0.567 | Требует улучшения |

**Классификационные задачи:**

| Задача | Лучшая модель | ROC-AUC | F1-Score | Качество |
|--------|---------------|---------|----------|----------|
| CC50 > median | RandomForest | 0.876 | 0.871 | Хорошее |
| IC50 > median | RandomForest | ~0.891 | ~0.886 | Хорошее |
| SI > median | RandomForest | 0.923 | 0.918 | Отличное |
| SI > 8 | RandomForest | ~0.908 | ~0.902 | Отличное |

### 3.2 Анализ производительности моделей

**Выводы:**
1. **RandomForest доминирует** во всех задачах
2. **Классификационные задачи** показывают лучшие результаты, чем регрессионные
3. **SI - наиболее предсказуемая** целевая переменная 
4. **Качество классификации** значительно превосходит качество регрессии

### 3.3 Рекомендации по улучшению

#### Для регрессионных задач:
1. **Feature Engineering:**
   - Создание полиномиальных признаков  
   - Взаимодействие между признаками
   - Домен-специфичные трансформации

2. **Ансамблевые методы:**
   - Stacking различных алгоритмов
   - Blending моделей
   - Weighted averaging

3. **Гиперпараметрическая оптимизация:**
   - Bayesian Optimization
   - Расширенный поиск по сетке
   - Random Search с большим количеством итераций

4. **Дополнительные алгоритмы:**
   - Gradient Boosting (LightGBM, CatBoost)
   - Support Vector Regression
   - Neural Networks

#### Для классификационных задач:
1. **Калибровка вероятностей** для улучшения интерпретируемости
2. **Threshold optimization** для максимизации бизнес-метрик
3. **Ensemble methods** для дальнейшего повышения качества
4. **Feature importance analysis** для понимания ключевых дескрипторов

---

## 4. ВЫВОДЫ

### 4.1 Основные достижения

1. **Успешная предобработка данных** с минимальными потерями (3.5%)
2. **Эффективное применение логарифмического преобразования** для улучшения распределений
3. **Достижение высокого качества классификации** (ROC-AUC > 0.87 для всех задач)
4. **Создание воспроизводимого пайплайна** машинного обучения
5. **Систематическое сравнение** различных алгоритмов с подбором гиперпараметров

### 4.2 Научная значимость

1. **Демонстрация эффективности** RandomForest для токсикологических данных
2. **Важность качественной предобработки** для биологических данных
3. **Превосходство классификационных** подходов над регрессионными для данного типа задач
4. **Валидация подхода** на реальных молекулярных дескрипторах

### 4.3 Практическая применимость

Разработанные модели могут быть использованы для:
- Предварительного скрининга новых химических соединений
- Приоритизации соединений для экспериментального тестирования
- Сокращения времени и затрат на разработку лекарственных препаратов
- Поддержки принятия решений в фармацевтической индустрии

---

## 5. ЗАКЛЮЧЕНИЕ

В рамках данной курсовой работы была продемонстрирована высокая эффективность применения методов машинного обучения для анализа токсикологических данных. Получены модели с высоким качеством предсказания, особенно для классификационных задач.

**Ключевые результаты:**
- **Лучшая модель классификации:** RandomForest с ROC-AUC = 0.923 (SI > median)
- **Лучшая модель регрессии:** RandomForest с R² = 0.418 (CC50)
- **Успешная обработка и анализ** 214 молекулярных дескрипторов
- **Воспроизводимый пайплайн** от сырых данных до готовых моделей

**Перспективы развития:**
1. **Deep Learning подходы** для работы с молекулярными структурами
2. **Интеграция дополнительных** типов биологических данных
3. **Разработка веб-интерфейса** для практического использования моделей
4. **Валидация на внешних** наборах данных
5. **Интерпретация модели** с помощью SHAP и LIME

---

## СТРУКТУРА ПРОЕКТА

```
MEPHI-ML/
├── data/
│   ├── data.xlsx                           # Исходные данные (1.4MB)
│   ├── processed_data.parquet              # Предобработанные данные (373KB)
│   ├── processed_data_full.parquet         # Полные обработанные данные (397KB)
│   └── targets.parquet                     # Целевые переменные (32KB)
├── eda.ipynb                               # Разведывательный анализ данных (862KB)
├── classification_cc50_over_median.ipynb   # Классификация CC50 > медиана (237KB)
├── classification_ic50_over_median.ipynb   # Классификация IC50 > медиана (261KB)
├── classification_si_over_median.ipynb     # Классификация SI > медиана (197KB)
├── classification_si_over_8.ipynb          # Классификация SI > 8 (196KB)
├── regression_cc50.ipynb                   # Регрессия CC50 (626KB)
├── regression_ic50.ipynb                   # Регрессия IC50 (574KB)
├── regression_si.ipynb                     # Регрессия SI (445KB)
├── README.md                               # Данный отчёт
```

---

**Технологический стек:** Python, scikit-learn, XGBoost, pandas, numpy, matplotlib, seaborn  
**Объём данных:** 966 соединений, 210+ молекулярных дескрипторов  
**Время выполнения:** II семестр обучения  
**Дата создания отчёта:** Январь 2025

---